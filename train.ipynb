{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Nature Conservancy Fisheries Monitoring Kaggle Competition\n",
    "\n",
    "This is an attempt to compete in the above mentioned Kaggle competition using a neural network written in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading settings...\n",
      "Settings loaded!\n"
     ]
    }
   ],
   "source": [
    "# Lets load us some settings!\n",
    "import json\n",
    "\n",
    "print \"Loading settings...\"\n",
    "with open('SETTINGS.json') as settings_file:\n",
    "    settings = json.load(settings_file)\n",
    "\n",
    "# Source directory for your data\n",
    "source_dir = settings['source_dir']\n",
    "train_dir = settings['train_dir']\n",
    "\n",
    "print \"Settings loaded!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish splitting train and val images!\n",
      "# training samples: 3019, # val samples: 758\n"
     ]
    }
   ],
   "source": [
    "# Credit goes to pengpaiSH for getting me started on the right path with this\n",
    "# Unnecessary to run this codeblock more than once.\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "root_train = source_dir + 'fish_train_set'\n",
    "root_val = source_dir + 'fish_val_set'\n",
    "\n",
    "root_total = source_dir + train_dir\n",
    "\n",
    "FishNames = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "nbr_train_samples = 0\n",
    "nbr_val_samples = 0\n",
    "\n",
    "# Training proportion\n",
    "split_proportion = 0.8\n",
    "\n",
    "for fish in FishNames:\n",
    "    if fish not in os.listdir(root_train):\n",
    "        os.mkdir(os.path.join(root_train, fish))\n",
    "\n",
    "    total_images = os.listdir(os.path.join(root_total, fish))\n",
    "\n",
    "    nbr_train = int(len(total_images) * split_proportion)\n",
    "\n",
    "    np.random.shuffle(total_images)\n",
    "\n",
    "    train_images = total_images[:nbr_train]\n",
    "\n",
    "    val_images = total_images[nbr_train:]\n",
    "\n",
    "    for img in train_images:\n",
    "        source = os.path.join(root_total, fish, img)\n",
    "        target = os.path.join(root_train, fish, img)\n",
    "        shutil.copy(source, target)\n",
    "        nbr_train_samples += 1\n",
    "\n",
    "    if fish not in os.listdir(root_val):\n",
    "        os.mkdir(os.path.join(root_val, fish))\n",
    "\n",
    "    for img in val_images:\n",
    "        source = os.path.join(root_total, fish, img)\n",
    "        target = os.path.join(root_val, fish, img)\n",
    "        shutil.copy(source, target)\n",
    "        nbr_val_samples += 1\n",
    "\n",
    "print('Finish splitting train and val images!')\n",
    "print('# training samples: {}, # val samples: {}'.format(nbr_train_samples, nbr_val_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3019 images belonging to 8 classes.\n",
      "Found 758 images belonging to 8 classes.\n",
      "Epoch 1/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.7811 - acc: 0.4505Epoch 00000: val_acc improved from -inf to 0.45383, saving model to ./weights.h5\n",
      "3019/3019 [==============================] - 367s - loss: 1.7814 - acc: 0.4501 - val_loss: 1.7572 - val_acc: 0.4538\n",
      "Epoch 2/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.7559 - acc: 0.4545Epoch 00001: val_acc improved from 0.45383 to 0.45910, saving model to ./weights.h5\n",
      "3019/3019 [==============================] - 348s - loss: 1.7550 - acc: 0.4554 - val_loss: 1.7387 - val_acc: 0.4591\n",
      "Epoch 3/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.7388 - acc: 0.4564Epoch 00002: val_acc improved from 0.45910 to 0.45910, saving model to ./weights.h5\n",
      "3019/3019 [==============================] - 351s - loss: 1.7397 - acc: 0.4558 - val_loss: 1.7291 - val_acc: 0.4591\n",
      "Epoch 4/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.7272 - acc: 0.4551Epoch 00003: val_acc did not improve\n",
      "3019/3019 [==============================] - 350s - loss: 1.7268 - acc: 0.4554 - val_loss: 1.7473 - val_acc: 0.4499\n",
      "Epoch 5/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.7227 - acc: 0.4545Epoch 00004: val_acc did not improve\n",
      "3019/3019 [==============================] - 343s - loss: 1.7222 - acc: 0.4554 - val_loss: 1.7198 - val_acc: 0.4472\n",
      "Epoch 6/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.7159 - acc: 0.4578Epoch 00005: val_acc did not improve\n",
      "3019/3019 [==============================] - 345s - loss: 1.7169 - acc: 0.4568 - val_loss: 1.7119 - val_acc: 0.4538\n",
      "Epoch 7/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.7145 - acc: 0.4551Epoch 00006: val_acc improved from 0.45910 to 0.46174, saving model to ./weights.h5\n",
      "3019/3019 [==============================] - 355s - loss: 1.7146 - acc: 0.4551 - val_loss: 1.7064 - val_acc: 0.4617\n",
      "Epoch 8/25\n",
      "3008/3019 [============================>.] - ETA: 0s - loss: 1.7102 - acc: 0.4561Epoch 00007: val_acc improved from 0.46174 to 0.46570, saving model to ./weights.h5\n",
      "3019/3019 [==============================] - 347s - loss: 1.7108 - acc: 0.4561 - val_loss: 1.7242 - val_acc: 0.4657\n",
      "Epoch 9/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.7068 - acc: 0.4591Epoch 00008: val_acc did not improve\n",
      "3019/3019 [==============================] - 367s - loss: 1.7068 - acc: 0.4588 - val_loss: 1.7292 - val_acc: 0.4538\n",
      "Epoch 10/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.7040 - acc: 0.4581Epoch 00009: val_acc did not improve\n",
      "3019/3019 [==============================] - 363s - loss: 1.7040 - acc: 0.4581 - val_loss: 1.6742 - val_acc: 0.4617\n",
      "Epoch 11/25\n",
      "3008/3019 [============================>.] - ETA: 0s - loss: 1.6975 - acc: 0.4584Epoch 00010: val_acc did not improve\n",
      "3019/3019 [==============================] - 326s - loss: 1.6972 - acc: 0.4581 - val_loss: 1.6915 - val_acc: 0.4472\n",
      "Epoch 12/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6974 - acc: 0.4598Epoch 00011: val_acc did not improve\n",
      "3019/3019 [==============================] - 356s - loss: 1.6965 - acc: 0.4598 - val_loss: 1.6807 - val_acc: 0.4538\n",
      "Epoch 13/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6927 - acc: 0.4631Epoch 00012: val_acc did not improve\n",
      "3019/3019 [==============================] - 343s - loss: 1.6925 - acc: 0.4631 - val_loss: 1.6967 - val_acc: 0.4604\n",
      "Epoch 14/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6919 - acc: 0.4681Epoch 00013: val_acc improved from 0.46570 to 0.48813, saving model to ./weights.h5\n",
      "3019/3019 [==============================] - 344s - loss: 1.6917 - acc: 0.4684 - val_loss: 1.6610 - val_acc: 0.4881\n",
      "Epoch 15/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6899 - acc: 0.4644Epoch 00014: val_acc did not improve\n",
      "3019/3019 [==============================] - 367s - loss: 1.6904 - acc: 0.4644 - val_loss: 1.7003 - val_acc: 0.4657\n",
      "Epoch 16/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6863 - acc: 0.4737Epoch 00015: val_acc did not improve\n",
      "3019/3019 [==============================] - 388s - loss: 1.6865 - acc: 0.4737 - val_loss: 1.6800 - val_acc: 0.4763\n",
      "Epoch 17/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6873 - acc: 0.4701Epoch 00016: val_acc did not improve\n",
      "3019/3019 [==============================] - 373s - loss: 1.6867 - acc: 0.4710 - val_loss: 1.6689 - val_acc: 0.4670\n",
      "Epoch 18/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6831 - acc: 0.4671Epoch 00017: val_acc did not improve\n",
      "3019/3019 [==============================] - 366s - loss: 1.6831 - acc: 0.4674 - val_loss: 1.6754 - val_acc: 0.4578\n",
      "Epoch 19/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6813 - acc: 0.4684Epoch 00018: val_acc did not improve\n",
      "3019/3019 [==============================] - 374s - loss: 1.6813 - acc: 0.4690 - val_loss: 1.7057 - val_acc: 0.4802\n",
      "Epoch 20/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6811 - acc: 0.4684Epoch 00019: val_acc did not improve\n",
      "3019/3019 [==============================] - 375s - loss: 1.6809 - acc: 0.4687 - val_loss: 1.6627 - val_acc: 0.4749\n",
      "Epoch 21/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6850 - acc: 0.4734Epoch 00020: val_acc did not improve\n",
      "3019/3019 [==============================] - 372s - loss: 1.6852 - acc: 0.4733 - val_loss: 1.6759 - val_acc: 0.4591\n",
      "Epoch 22/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6747 - acc: 0.4744Epoch 00021: val_acc did not improve\n",
      "3019/3019 [==============================] - 373s - loss: 1.6749 - acc: 0.4743 - val_loss: 1.6632 - val_acc: 0.4710\n",
      "Epoch 23/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6761 - acc: 0.4678Epoch 00022: val_acc did not improve\n",
      "3019/3019 [==============================] - 366s - loss: 1.6761 - acc: 0.4677 - val_loss: 1.6580 - val_acc: 0.4670\n",
      "Epoch 24/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6760 - acc: 0.4674Epoch 00023: val_acc did not improve\n",
      "3019/3019 [==============================] - 368s - loss: 1.6755 - acc: 0.4680 - val_loss: 1.6582 - val_acc: 0.4749\n",
      "Epoch 25/25\n",
      "3008/3019 [============================>.] - ETA: 1s - loss: 1.6747 - acc: 0.4757Epoch 00024: val_acc did not improve\n",
      "3019/3019 [==============================] - 369s - loss: 1.6741 - acc: 0.4766 - val_loss: 1.6745 - val_acc: 0.4538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1073584d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from keras.layers import Flatten, Dense, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "root_train = source_dir + 'fish_train_set'\n",
    "root_val = source_dir + 'fish_val_set'\n",
    "\n",
    "# Credit goes to pengpaiSH for this section\n",
    "learning_rate = 0.0001\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "nbr_train_samples = 3019\n",
    "nbr_validation_samples = 758\n",
    "nbr_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = root_train\n",
    "val_data_dir = root_val\n",
    "\n",
    "FishNames = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "### Finally, this is me.  This is also the guts of the model.\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(299, 299, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(GaussianNoise(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "###\n",
    "\n",
    "# autosave best Model\n",
    "best_model_file = \"./weights.h5\"\n",
    "best_model = ModelCheckpoint(best_model_file, monitor='val_acc', verbose = 1, save_best_only = True)\n",
    "\n",
    "# Credit goes to pengpaiSH for this section\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size = (img_width, img_height),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        classes = FishNames,\n",
    "        class_mode = 'categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle = True,\n",
    "        classes = FishNames,\n",
    "        class_mode = 'categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nbr_train_samples,\n",
    "        nb_epoch = nbr_epochs,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nbr_validation_samples,\n",
    "        callbacks = [best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Loading model and weights from training process ...\n",
      "Begin to predict for testing data ...\n",
      "Begin to write submission file ..\n",
      "0 / 1000\n",
      "100 / 1000\n",
      "200 / 1000\n",
      "300 / 1000\n",
      "400 / 1000\n",
      "500 / 1000\n",
      "600 / 1000\n",
      "700 / 1000\n",
      "800 / 1000\n",
      "900 / 1000\n",
      "Submission file successfully generated!\n"
     ]
    }
   ],
   "source": [
    "# Credit goes to pengpaiSH for most of this section\n",
    "\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "batch_size = 32\n",
    "nbr_test_samples = 1000\n",
    "\n",
    "FishNames = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "root_path = './'\n",
    "\n",
    "\n",
    "weights_path = os.path.join(root_path, 'weights.h5')\n",
    "\n",
    "test_data_dir = os.path.join(source_dir, 'test_stg1/')\n",
    "\n",
    "# test data generator for prediction\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False, # Important !!!\n",
    "        classes = None,\n",
    "        class_mode = None)\n",
    "\n",
    "test_image_list = test_generator.filenames\n",
    "\n",
    "print('Loading model and weights from training process ...')\n",
    "model = load_model(weights_path)\n",
    "\n",
    "print('Begin to predict for testing data ...')\n",
    "predictions = model.predict_generator(test_generator, nbr_test_samples)\n",
    "\n",
    "np.savetxt(os.path.join(root_path, 'predictions.txt'), predictions)\n",
    "\n",
    "\n",
    "print('Begin to write submission file ..')\n",
    "f_submit = open(os.path.join(root_path, 'submit.csv'), 'w')\n",
    "f_submit.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "for i, image_name in enumerate(test_image_list):\n",
    "    pred = ['%.6f' % p for p in predictions[i, :]]\n",
    "    if i % 100 == 0:\n",
    "        print('{} / {}'.format(i, nbr_test_samples))\n",
    "    f_submit.write('%s,%s\\n' % (os.path.basename(image_name), ','.join(pred)))\n",
    "\n",
    "f_submit.close()\n",
    "\n",
    "print('Submission file successfully generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission History\n",
    "* 2016-12-10 @ ~22:30: 1.70565 (Worse than sample submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
