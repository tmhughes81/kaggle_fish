{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Nature Conservancy Fisheries Monitoring Kaggle Competition\n",
    "\n",
    "This is an attempt to compete in the above mentioned Kaggle competition using a neural network written in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading settings...\n",
      "Settings loaded!\n"
     ]
    }
   ],
   "source": [
    "# Lets load us some settings!\n",
    "import json\n",
    "\n",
    "print \"Loading settings...\"\n",
    "with open('SETTINGS.json') as settings_file:\n",
    "    settings = json.load(settings_file)\n",
    "\n",
    "# Source directory for your data\n",
    "source_dir = settings['source_dir']\n",
    "train_dir = settings['train_dir']\n",
    "\n",
    "print \"Settings loaded!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish splitting train and val images!\n",
      "# training samples: 3019, # val samples: 758\n"
     ]
    }
   ],
   "source": [
    "# Credit goes to pengpaiSH for getting me started on the right path with this\n",
    "# Unnecessary to run this codeblock more than once.\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "root_train = source_dir + 'fish_train_set'\n",
    "root_val = source_dir + 'fish_val_set'\n",
    "\n",
    "root_total = source_dir + train_dir\n",
    "\n",
    "FishNames = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "nbr_train_samples = 0\n",
    "nbr_val_samples = 0\n",
    "\n",
    "# Training proportion\n",
    "split_proportion = 0.8\n",
    "\n",
    "for fish in FishNames:\n",
    "    if fish not in os.listdir(root_train):\n",
    "        os.mkdir(os.path.join(root_train, fish))\n",
    "\n",
    "    total_images = os.listdir(os.path.join(root_total, fish))\n",
    "\n",
    "    nbr_train = int(len(total_images) * split_proportion)\n",
    "\n",
    "    np.random.shuffle(total_images)\n",
    "\n",
    "    train_images = total_images[:nbr_train]\n",
    "\n",
    "    val_images = total_images[nbr_train:]\n",
    "\n",
    "    for img in train_images:\n",
    "        source = os.path.join(root_total, fish, img)\n",
    "        target = os.path.join(root_train, fish, img)\n",
    "        shutil.copy(source, target)\n",
    "        nbr_train_samples += 1\n",
    "\n",
    "    if fish not in os.listdir(root_val):\n",
    "        os.mkdir(os.path.join(root_val, fish))\n",
    "\n",
    "    for img in val_images:\n",
    "        source = os.path.join(root_total, fish, img)\n",
    "        target = os.path.join(root_val, fish, img)\n",
    "        shutil.copy(source, target)\n",
    "        nbr_val_samples += 1\n",
    "\n",
    "print('Finish splitting train and val images!')\n",
    "print('# training samples: {}, # val samples: {}'.format(nbr_train_samples, nbr_val_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3019 images belonging to 8 classes.\n",
      "Found 758 images belonging to 8 classes.\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "root_train = source_dir + 'fish_train_set'\n",
    "root_val = source_dir + 'fish_val_set'\n",
    "\n",
    "# Credit goes to pengpaiSH for this section\n",
    "learning_rate = 0.0001\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "nbr_train_samples = 3019\n",
    "nbr_validation_samples = 758\n",
    "nbr_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = root_train\n",
    "val_data_dir = root_val\n",
    "\n",
    "FishNames = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "### Finally, this is me.  This is also the guts of the model.\n",
    "model = Sequential()\n",
    "\n",
    "### Model 2\n",
    "model.add(Convolution2D(128, 5, 5, input_shape=(299, 299, 3)))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Convolution2D(16, 2, 2))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(8))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# autosave best Model\n",
    "best_model_file = \"./weights.h5\"\n",
    "best_model = ModelCheckpoint(best_model_file, monitor='val_acc', verbose = 1, save_best_only = True)\n",
    "\n",
    "# Credit goes to pengpaiSH for this section\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size = (img_width, img_height),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        classes = FishNames,\n",
    "        class_mode = 'categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle = True,\n",
    "        classes = FishNames,\n",
    "        class_mode = 'categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nbr_train_samples,\n",
    "        nb_epoch = nbr_epochs,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nbr_validation_samples,\n",
    "        callbacks = [best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Loading model and weights from training process ...\n",
      "Begin to predict for testing data ...\n",
      "0th augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "Begin to predict for testing data ...\n",
      "1th augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "Begin to predict for testing data ...\n",
      "2th augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "Begin to predict for testing data ...\n",
      "3th augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "Begin to predict for testing data ...\n",
      "4th augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "Begin to predict for testing data ...\n",
      "Begin to write submission file ..\n",
      "0 / 1000\n",
      "100 / 1000\n",
      "200 / 1000\n",
      "300 / 1000\n",
      "400 / 1000\n",
      "500 / 1000\n",
      "600 / 1000\n",
      "700 / 1000\n",
      "800 / 1000\n",
      "900 / 1000\n",
      "Submission file successfully generated!\n"
     ]
    }
   ],
   "source": [
    "# Credit goes to pengpaiSH for most of this section\n",
    "\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "batch_size = 32\n",
    "nbr_test_samples = 1000\n",
    "nbr_augmentation = 5\n",
    "\n",
    "FishNames = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "root_path = './'\n",
    "\n",
    "\n",
    "weights_path = os.path.join(root_path, 'weights.h5')\n",
    "\n",
    "test_data_dir = os.path.join(source_dir, 'test_stg1/')\n",
    "\n",
    "# test data generator for prediction\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False, # Important !!!\n",
    "        classes = None,\n",
    "        class_mode = None)\n",
    "\n",
    "test_image_list = test_generator.filenames\n",
    "\n",
    "print('Loading model and weights from training process ...')\n",
    "model = load_model(weights_path)\n",
    "\n",
    "for idx in range(nbr_augmentation):\n",
    "    print('{}th augmentation for testing ...'.format(idx))\n",
    "    \n",
    "    random_seed = idx\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "            test_data_dir,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=batch_size,\n",
    "            shuffle = False, # Important !!!\n",
    "            seed = random_seed,\n",
    "            classes = None,\n",
    "            class_mode = None)\n",
    "\n",
    "    test_image_list = test_generator.filenames\n",
    "    #print('image_list: {}'.format(test_image_list[:10]))\n",
    "    print('Begin to predict for testing data ...')\n",
    "    if idx == 0:\n",
    "        predictions = model.predict_generator(test_generator, nbr_test_samples)\n",
    "    else:\n",
    "        predictions += model.predict_generator(test_generator, nbr_test_samples)\n",
    "\n",
    "predictions /= nbr_augmentation\n",
    "\n",
    "print('Begin to write submission file ..')\n",
    "f_submit = open(os.path.join(root_path, 'submit.csv'), 'w')\n",
    "f_submit.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "for i, image_name in enumerate(test_image_list):\n",
    "    pred = ['%.6f' % p for p in predictions[i, :]]\n",
    "    if i % 100 == 0:\n",
    "        print('{} / {}'.format(i, nbr_test_samples))\n",
    "    f_submit.write('%s,%s\\n' % (os.path.basename(image_name), ','.join(pred)))\n",
    "\n",
    "f_submit.close()\n",
    "\n",
    "print('Submission file successfully generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission History\n",
    "* 2016-12-10 @ ~22:30: 1.70565 (Worse than sample submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
